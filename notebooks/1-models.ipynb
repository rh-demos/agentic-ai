{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8d1bbb-aa1e-48ba-b028-9e9a7df7ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q dotenv llama_stack_client==0.3.5 fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718e276a-e5a1-4ed8-bbed-67f2be690b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import rich\n",
    "\n",
    "from llama_stack_client import LlamaStackClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4699d49-684a-4ccb-a01f-91556388ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://localhost:8321\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151d05b9-f7e1-4644-904c-cec0a9d09925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_stream(chunks) -> None:\n",
    "    for chunk in chunks:\n",
    "        if not chunk.choices:\n",
    "            continue\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f55c822-acf5-4140-a527-559422b926a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-distribution-vllm-service:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked, \"Hello, who are you?\" I need to respond appropriately. First, I should introduce myself as Qwen, the large language model developed by Alibaba Cloud. I should mention my capabilities, like answering questions, creating content, and assisting with various tasks. I should keep the tone friendly and open-ended to encourage further interaction. Let me make sure the response is clear and concise, avoiding any technical jargon. Also, I should check if there's any specific information the user might be looking for beyond the basic introduction. Maybe they want to know about my training data or use cases. I should invite them to ask more questions or share their needs. Let me structure the response step by step: greeting, introduction, capabilities, invitation to interact. That should cover it.\n",
      "</think>\n",
      "\n",
      "Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I'm designed to assist with a wide range of tasks, such as answering questions, creating content, solving problems, and more. Whether you need help with writing, coding, learning, or just chatting, I'm here to provide support! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, who are you?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "stream = True\n",
    "model = \"vllm/qwen3-8b\"\n",
    "\n",
    "if stream:\n",
    "    response_stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    _print_stream(response_stream)\n",
    "    response_stream.close()\n",
    "else:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779f32f-2887-43a0-9bdc-fc9ad5e41a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
