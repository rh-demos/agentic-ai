apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  annotations:
    openshift.io/display-name: llamastack-distribution-vllm
  name: llamastack-distribution-vllm
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: VLLM_API_URL
          value: 'http://qwen3-8b-predictor.agentic-ai.svc.cluster.local:8080/v1'
        - name: VLLM_API_TOKEN
          value: 'your_token'
        - name: VLLM_TLS_VERIFY
          value: 'false'
        - name: VLLM_MAX_TOKENS
          value: '4096'
        - name: GUARD_API_URL
          value: http://llama-guard-3-1b-predictor:8080/v1
        - name: GUARD_API_TOKEN
          value: 'your_token'
        - name: GUARD_TLS_VERIFY
          value: 'false'
        - name: GUARD_MAX_TOKENS
          value: '4096'
        - name: FMS_ORCHESTRATOR_URL
          value: 'https://guardrails-orchestrator-service:8032'
        - name: FMS_VERIFY_SSL
          value: 'false'
        - name: LLAMA_STACK_LOGGING
          value: all=info
      name: llama-stack
      port: 8321
      resources:
        limits:
          cpu: '2'
          memory: 12Gi
        requests:
          cpu: 250m
          memory: 500Mi
    distribution:
      name:  rh-dev
    userConfig:
      configMapName: llama-stack-config
    storage:
      size: 20Gi
      mountPath: /opt/app-root/src