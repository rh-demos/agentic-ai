kind: ConfigMap
apiVersion: v1
metadata:
  name: llama-stack-config
  labels:
    llamastack.io/distribution: llamastack-dist-agentic
    opendatahub.io/dashboard: 'true'
data:
  run.yaml: |
    # Llama Stack Configuration
    version: "2"
    image_name: remote-vllm
    apis:
    - agents
    - datasetio
    - eval
    - files
    - inference
    - safety
    - scoring
    - tool_runtime
    - vector_io
    providers:
      inference:
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      - provider_id: vllm
        provider_type: remote::vllm
        config:
          url: ${env.VLLM_API_URL:=fake}
          api_token: ${env.VLLM_API_TOKEN:=fake}
          tls_verify: ${env.VLLM_TLS_VERIFY:=false}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
      - provider_id: llama-guard
        provider_type: remote::vllm
        config:
          url: ${env.GUARD_API_URL:=fake}
          api_token: ${env.GUARD_API_TOKEN:=fake}
          tls_verify: ${env.GUARD_TLS_VERIFY:=false}
          max_tokens: ${env.GUARD_MAX_TOKENS:=4096}
      vector_io:
      - provider_id: milvus
        provider_type: inline::milvus
        config:
          db_path: /opt/app-root/src/.llama/distributions/rh/milvus.db
          kvstore:
            backend: default
            namespace: "milvus"
          persistence:
            backend: default
            namespace: "milvus_persistence"
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence:
            agent_state:
              backend: default
              namespace: "agent_state"
            responses:
              backend: sql
              namespace: "agents"
              table_name: "responses"
      eval:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          kvstore:
            backend: default
            namespace: "eval"
      safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config: {} 
      - provider_id: trustyai_fms
        provider_type: remote::trustyai_fms
        module: llama_stack_provider_trustyai_fms
        config:
          orchestrator_url: ${env.FMS_ORCHESTRATOR_URL:=fake}
          shields:
            pii:
              type: content
              confidence_threshold: 0.5
              message_types: ["user", "system"]
              detectors:
                  regex:
                    detector_params: 
                      regex: ["email", "ssn", "credit-card", "^hello$", "\\b[A-Z]{1,2}[0-9]{6}\\(?[0-9A]\\)?\\b"]
            hap:
              type: content
              confidence_threshold: 0.5
              message_types: ["user", "system"]
              detectors:
                  hap: 
                    detector_params: {}
            prompt_injection:
              type: content
              confidence_threshold: 0.5
              message_types: ["user", "system"]
              detectors:
                  prompt_injection: 
                    detector_params: {}
      files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          metadata_store:
            backend: sql
            namespace: "files"
            table_name: "files_metadata"
          storage_dir: /opt/app-root/src/.llama/distributions/rh/files
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            backend: default
            namespace: "huggingface"
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          kvstore:
            backend: default
            namespace: "localfs"
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      tool_runtime:
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:=}
          max_results: 3        
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      backend: default
      namespace: "metadata"
    storage:
      backends:
        default:
          type: kv_sqlite
          db_path: /opt/app-root/src/.llama/distributions/rh/storage.db
          namespace: null
        sql:
          type: sql_sqlite
          db_path: /opt/app-root/src/.llama/distributions/rh/sql_storage.db
      stores:
        metadata:
          backend: default
          namespace: "llama-stack"
        conversations:
          backend: sql
          namespace: "conversations"
          table_name: "conversations"
        inference:
          backend: sql
          namespace: "inference"
          table_name: "inference"
    registered_resources:
      models:
      - provider_id: sentence-transformers
        model_id: granite-embedding-125m
        provider_model_id: ibm-granite/granite-embedding-125m-english
        model_type: embedding
        metadata:
          embedding_dimension: 768

      shields:
        - shield_id: pii
          provider_id: trustyai_fms
        - shield_id: pii
          provider_id: trustyai_fms
        - shield_id: hap
          provider_id: trustyai_fms
        - shield_id: prompt_injection
          provider_id: trustyai_fms
        - shield_id: content_safety
          provider_id: llama-guard
          provider_shield_id: llama-guard/llama-guard-3-1b
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      - toolgroup_id: builtin::websearch
        provider_id: tavily-search      
    server:
      port: 8321
